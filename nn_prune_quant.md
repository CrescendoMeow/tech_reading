# Neural Network Model Pruning and Quantization

## Equalization

- [ICML 2019] [Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization](https://openaccess.thecvf.com/content_ICCV_2019/papers/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.pdf)
- [ICCV 2019] [Data-Free Quantization Through Weight Equalization and Bias Correction](https://openaccess.thecvf.com/content_ICCV_2019/papers/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.pdf)

## Lottery Ticket Hypothesis

- [ICLR 2019] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks[[Arxiv]](https://arxiv.org/abs/1803.03635)[[OpenReview]](https://openreview.net/forum?id=rJl-b3RcF7)
- [ICLR 2020] Comparing Rewinding and Fine-tuning in Neural Network Pruning [[Arxiv]](https://arxiv.org/abs/2003.02389)[[OpenReview]](https://openreview.net/forum?id=S1gSj0NKvB)
